{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc41e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eeb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置\n",
    "DATA_DIR = r\"data\\Air pollution data\\FI\"\n",
    "URLS_CSV = r\"data\\Air pollution data\\FI\\ParquetFilesUrlse1a.csv\"\n",
    "\n",
    "# 设置固定日期范围\n",
    "START_DATE = \"2020-10-01\"\n",
    "END_DATE = \"2025-09-30\"\n",
    "# 预测开始日期\n",
    "PREDICTION_START_DATE = \"2025-10-01\"\n",
    "\n",
    "print(f'Fetching data from {START_DATE} to {END_DATE}')\n",
    "print(f'Prediction will start from {PREDICTION_START_DATE} for one year')\n",
    "\n",
    "# 下载parquet文件的函数\n",
    "def download_parquet_files(urls_csv, data_dir):\n",
    "    # 从CSV文件读取URLs\n",
    "    urls = pd.read_csv(urls_csv)[\"ParquetFileUrl\"]\n",
    "    # 下载每个文件\n",
    "    for url in urls:\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        # 只在文件不存在时下载\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f'Downloading {file_name}...')\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            except Exception as e:\n",
    "                print(f'Error downloading {file_name}: {e}')\n",
    "        else:\n",
    "            print(f'{file_name} already exists, skipping download.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331baa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和预处理空气污染数据的函数\n",
    "def load_air_pollution_data(data_dir, start_date, end_date):\n",
    "    # 定义要读取的列\n",
    "    columns_air = [\"Samplingpoint\", \"Pollutant\", \"Start\", \"Value\", \"Unit\", \"Validity\"]\n",
    "    \n",
    "    # 初始化空DataFrame\n",
    "    df_air = pd.DataFrame()\n",
    "    \n",
    "    # 遍历目录并读取所有parquet文件\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.parquet'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    # 读取parquet文件\n",
    "                    data = pd.read_parquet(file_path, columns=columns_air)\n",
    "                    # 合并到主DataFrame\n",
    "                    df_air = pd.concat([df_air, data])\n",
    "                except Exception as e:\n",
    "                    print(f'Error reading {filename}: {e}')\n",
    "    \n",
    "    # 数据清洗和预处理\n",
    "    if not df_air.empty:\n",
    "        # 将Start列转换为datetime类型\n",
    "        df_air['Start'] = pd.to_datetime(df_air['Start'])\n",
    "        \n",
    "        # 按日期范围过滤数据\n",
    "        df_air = df_air[(df_air['Start'] >= start_date) & (df_air['Start'] <= end_date)]\n",
    "        \n",
    "        # 只保留有效数据\n",
    "        df_air = df_air[df_air['Validity'] == 1]\n",
    "        \n",
    "        # 移除Validity列，因为不再需要\n",
    "        df_air = df_air.drop(['Validity'], axis=1)\n",
    "        \n",
    "        # 从Samplingpoint中移除国家代码前缀\n",
    "        if 'Samplingpoint' in df_air.columns:\n",
    "            df_air['Samplingpoint'] = df_air['Samplingpoint'].str[3:]\n",
    "        \n",
    "        # 将Value转换为float并处理负值\n",
    "        df_air['Value'] = pd.to_numeric(df_air['Value'], errors='coerce')\n",
    "        df_air['Value'] = df_air['Value'].where(lambda x: x > 0, np.nan)\n",
    "        \n",
    "        # 前向填充缺失值\n",
    "        df_air = df_air.ffill()\n",
    "        \n",
    "        # 删除任何剩余的NaN值\n",
    "        df_air = df_air.dropna()\n",
    "    \n",
    "    return df_air\n",
    "\n",
    "# 执行数据获取\n",
    "print('Downloading parquet files...')\n",
    "download_parquet_files(URLS_CSV, DATA_DIR)\n",
    "\n",
    "print('\\nLoading and preprocessing air pollution data...')\n",
    "df_air = load_air_pollution_data(DATA_DIR, START_DATE, END_DATE)\n",
    "\n",
    "print(f'\\nLoaded {len(df_air)} records')\n",
    "if not df_air.empty:\n",
    "    print('\\nSample data:')\n",
    "    print(df_air.head())\n",
    "    print('\\nData summary:')\n",
    "    print(df_air.describe())\n",
    "    \n",
    "    # 获取所有唯一站点\n",
    "    stations = df_air['Samplingpoint'].unique()\n",
    "    print(f'\\nFound {len(stations)} unique sampling stations:')\n",
    "    print(stations)\n",
    "\n",
    "# 为GRU准备多污染物时间序列数据的函数\n",
    "def prepare_multi_pollutant_data(df, pollutant_ids, station_id, sequence_length=24):\n",
    "    \"\"\n",
    "    # 按站点过滤数据\n",
    "    df_station = df[df['Samplingpoint'] == station_id].copy()\n",
    "    \n",
    "    # 确保数据按时间排序\n",
    "    df_station = df_station.sort_values('Start')\n",
    "    \n",
    "    # 获取唯一的日期时间点\n",
    "    timestamps = df_station['Start'].unique()\n",
    "    timestamps.sort()\n",
    "    \n",
    "    # 创建一个时间点索引的数据框，用于合并不同污染物的数据\n",
    "    multi_pollutant_df = pd.DataFrame({'Start': timestamps})\n",
    "    \n",
    "    # 为每个污染物创建一列\n",
    "    for pollutant_id in pollutant_ids:\n",
    "        pollutant_data = df_station[df_station['Pollutant'] == pollutant_id][['Start', 'Value']]\n",
    "        pollutant_data = pollutant_data.rename(columns={'Value': f'Value_{pollutant_id}'})\n",
    "        multi_pollutant_df = pd.merge(multi_pollutant_df, pollutant_data, on='Start', how='left')\n",
    "    \n",
    "    # 填充缺失值\n",
    "    multi_pollutant_df = multi_pollutant_df.ffill().bfill()\n",
    "    multi_pollutant_df = multi_pollutant_df.dropna()\n",
    "    \n",
    "    # 准备数据用于时间序列预测\n",
    "    X, y, dates = [], [], []\n",
    "    feature_cols = [f'Value_{pid}' for pid in pollutant_ids]\n",
    "    num_features = len(pollutant_ids)\n",
    "    \n",
    "    # 为每个特征创建单独的缩放器\n",
    "    scalers = {pid: MinMaxScaler(feature_range=(0, 1)) for pid in pollutant_ids}\n",
    "    \n",
    "    # 对每个污染物进行归一化\n",
    "    scaled_values = np.zeros((len(multi_pollutant_df), num_features))\n",
    "    for i, pid in enumerate(pollutant_ids):\n",
    "        values_col = multi_pollutant_df[f'Value_{pid}'].values.reshape(-1, 1)\n",
    "        scaled_values[:, i] = scalers[pid].fit_transform(values_col).flatten()\n",
    "    \n",
    "    # 创建时间序列序列\n",
    "    for i in range(len(scaled_values) - sequence_length):\n",
    "        X.append(scaled_values[i:i+sequence_length])\n",
    "        y.append(scaled_values[i+sequence_length])\n",
    "        dates.append(multi_pollutant_df.iloc[i+sequence_length]['Start'])\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y, scalers, dates\n",
    "\n",
    "# 自定义PyTorch数据集类\n",
    "class MultiPollutantDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# GRU模型定义 - 支持多输出\n",
    "class MultiOutputGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=4):\n",
    "        super(MultiOutputGRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU层\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # 全连接层 - 输出多个污染物的值\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 初始化为零的隐藏状态\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # 前向传播GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # 将最后一个时间步的输出传递给全连接层\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, test_loader, num_epochs=50, learning_rate=0.001):\n",
    "    # 设置设备（可用GPU时使用GPU，否则使用CPU）\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 训练历史\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # 计算平均训练损失\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # 在测试集上评估\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # 计算平均测试损失\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "# 如果有数据则训练模型\n",
    "if 'X_train' in locals():\n",
    "    print('Initializing and training GRU model...')\n",
    "    \n",
    "    # 创建模型实例\n",
    "    model = GRUModel(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
    "    \n",
    "    # 训练模型\n",
    "    model, train_losses, test_losses = train_model(\n",
    "        model, train_loader, test_loader, num_epochs=50, learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # 绘制训练和测试损失\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('GRU Model Training Progress')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a62108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型并进行预测的函数\n",
    "def evaluate_model(model, test_loader, scalers, pollutant_ids):\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 存储预测和实际值的字典\n",
    "    all_predictions = {pid: [] for pid in pollutant_ids}\n",
    "    all_actuals = {pid: [] for pid in pollutant_ids}\n",
    "    \n",
    "    # 在测试集上进行预测\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            \n",
    "            # 转换为numpy并反向转换\n",
    "            predictions_np = predictions.cpu().numpy()\n",
    "            y_batch_np = y_batch.numpy()\n",
    "            \n",
    "            # 反向转换以获取实际值\n",
    "            predictions_inv = scaler.inverse_transform(predictions_np.reshape(-1, 1)).flatten()\n",
    "            actuals_inv = scaler.inverse_transform(y_batch_np.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            all_predictions.extend(predictions_inv)\n",
    "            all_actuals.extend(actuals_inv)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = mean_squared_error(all_actuals, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(all_actuals, all_predictions)\n",
    "    \n",
    "    print(f'Model Evaluation Metrics:')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    \n",
    "    return all_predictions, all_actuals, rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed917d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果模型存在则评估\n",
    "if 'model' in locals():\n",
    "    print('Evaluating model performance...')\n",
    "    predictions, actuals, rmse, mae = evaluate_model(model, test_loader, scaler)\n",
    "    \n",
    "    # 绘制预测值与实际值对比\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_dates[:len(predictions)], actuals, label='Actual Values', color='blue')\n",
    "    plt.plot(test_dates[:len(predictions)], predictions, label='Predictions', color='red', linestyle='--')\n",
    "    plt.title('GRU Model Predictions vs Actual Values')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Pollutant Concentration')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测未来值的函数\n",
    "def predict_future(model, last_sequence, scaler, pollutant_ids, prediction_days=365):\n",
    "    # 设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 将last_sequence转换为tensor\n",
    "    current_sequence = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "    current_sequence = current_sequence.unsqueeze(0)  # 添加批次维度\n",
    "    \n",
    "    # 存储预测的列表\n",
    "    future_predictions = []\n",
    "    \n",
    "    # 为指定天数生成预测\n",
    "    for _ in range(prediction_days * 24):  # 假设每小时预测\n",
    "        with torch.no_grad():\n",
    "            # 预测下一个值\n",
    "            next_pred = model(current_sequence)\n",
    "            \n",
    "            # 转换为numpy并存储\n",
    "            next_pred_np = next_pred.cpu().numpy()[0][0]\n",
    "            future_predictions.append(next_pred_np)\n",
    "            \n",
    "            # 更新序列以进行下一次预测\n",
    "            new_sequence = torch.cat([\n",
    "                current_sequence[:, 1:, :],\n",
    "                next_pred.unsqueeze(2)\n",
    "            ], dim=1)\n",
    "            current_sequence = new_sequence\n",
    "    \n",
    "    # 反向转换以获取实际值\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    future_predictions_inv = scaler.inverse_transform(future_predictions).flatten()\n",
    "    \n",
    "    return future_predictions_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014676cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果模型存在则进行未来预测\n",
    "if 'model' in locals() and 'X_test' in locals() and 'scaler' in locals():\n",
    "    print('Making predictions for the current year...')\n",
    "    \n",
    "    # 获取测试数据中的最后一个序列\n",
    "    last_sequence = X_test[-1]\n",
    "    \n",
    "    # 预测今年（简化为365天）\n",
    "    current_year_predictions = predict_future(model, last_sequence, scaler, prediction_days=300)  # 演示使用30天\n",
    "    \n",
    "    prediction_start = datetime.strptime(PREDICTION_START_DATE, '%Y-%m-%d')\n",
    "    future_dates = [prediction_start + timedelta(hours=i) for i in range(len(current_year_predictions))]\n",
    "    \n",
    "    # 绘制未来预测\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(future_dates, current_year_predictions, color='green', label='Predicted Values for Current Year')\n",
    "    plt.title('GRU Model Predictions for Current Year')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Pollutant Concentration')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        # 保存预测到CSV\n",
    "    pred_df = pd.DataFrame({\n",
    "        'Date': future_dates,\n",
    "        'Predicted_Pollutant_Level': current_year_predictions\n",
    "    })\n",
    "    \n",
    "    pred_csv_path = 'current_year_predictionsv0.5.csv'\n",
    "    pred_df.to_csv(pred_csv_path, index=False)\n",
    "    print(f'Predictions saved to {pred_csv_path}')\n",
    "    \n",
    "    # 显示预测的汇总统计\n",
    "    print('\\nSummary of Current Year Predictions:')\n",
    "    print(f'Minimum predicted level: {current_year_predictions.min():.2f}')\n",
    "    print(f'Maximum predicted level: {current_year_predictions.max():.2f}')\n",
    "    print(f'Average predicted level: {current_year_predictions.mean():.2f}')\n",
    "    print(f'Standard deviation: {current_year_predictions.std():.2f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ab5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
